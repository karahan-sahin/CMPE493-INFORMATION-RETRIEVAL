# Term Weighting and Ranked Retrieval Models (Continued)



## Term frequency **tf**

- The term frequency tf~t,d~ of term *t* in document d is defined as the number of times that *t* occurs in document *d*
- We want to use tf when computing query-document match scores. But how?
- Raw term frequency is not what we want:
  - A document with 10 occurrences of the term is more relevant than a document with 1 occurrence of the term
  - **But not 10 times more relevant**
- ==**Relevance** does not increase proportionally with **term frequency**.==

### Log-frequency weighting

- The log frequency weight of term *t* in *d* is

$$
w_{td}=\begin{Bmatrix}
1 + \log_{10}tf_{t,d}, & if\space tf_{t,d} > 0  \\ 0, & otherwise  \end{Bmatrix}
$$

- 0 -->  1; 1 --> 1; 2 --> 1.3; 10 --> 2; 1000 --> 4; .....
- **Score for a document-query pair**: sum over terms *t* in both *q* and *d*:

$$
Score = \sum_{t \in q \cap d}(1+\log\space tf_{t,d})
$$

- Terms which are common in the query and the document
- The score is 0 if none of the query terms is present in the document

### Document frequency

- Rare terms are more informative than frequent terms
  
- Recall stop words
  
- Consider a term that is rare in the collection (e.g., arachnophobic)

- A document containing this term is very likely to be relevant to the query *arachnophobic*

- We want **a high weight for rare terms** like arachnophobic

  

- Frequent terms are less informative than rare items 

- Consider a query term that is frequent in the collection (eg *high, increase, and line*)

- A document containing such a term is more likely to be relevant than a document that doesn't

- But it's not a sure indicator of relevance.

- **For frequent terms**, we want **high positive weights** for words like *high, increase and line*

  - But **lower weights than for rare terms**

- We will use document frequency(df) to capture this.

### idf(Inverse Document Frequency) weight

- df~t~ is the <u>document</u> frequency of *t* : the number of documents that contain *t*
  - ``word.df = len(word.inverted_index)``
  - df~t~ is an inverse measure of the informativeness of *t*
  - df~t~ <u><</u> N (where **N is the total number of documents in the collection**)
- We define the idf(inverse document frequency) of *t* by

$$
idf_{t} = \log_{10}(N/df_{t})
$$

- We use log(N/df~t~) instead of N/df~t~ to "dampen" the effect of idf.
  - ``word.idf = log10(len(collection)/word.df)``
- When it doesn't make much different in 1+ tf~t~ for example
  - Low number of Collections
  - Much content in document
- Normalization?

#### idf example, suppose N = 1 million

| term      | df~t~                          | idf~t~ |
| --------- | ------------------------------ | ------ |
| calpurnia | 1 = log(10^6^ . 10^0^)         | 6      |
| animal    | 100 = log(10^6^. 10^-2^)       | 4      |
| sunday    | 1000 = log(10^6^ .10^-3^)      | 3      |
| fly       | 10,000 = log(10^6^ .10^-4^)    | 2      |
| under     | 100,000 = log(10^6^ .10^-5^)   | 1      |
| the       | 1,000,000 = log(10^6^ .10^-6^) | 0      |

$$ {textwidth=10cm}
idf_{t} = \log_{10}(N/ df_{t})
$$

- There is one idf value for each term *t* in a collection.

#### Effects of idf on ranking

- Does idf have an effect on ranking one-term queries, like
  - Arachnophobic
- idf has no effect on ranking one term queries
  - idf affects the ranking of documents for queries with at least two terms
  - For the query *arachnophobic person*, idf weighting makes occurrences of *arachnophobic* count for **more** in the **document ranking** than occurrences of *person*

### Collection vs. Document frequency

- The collection frequency of *t* is the number of occurrences of *t* in the collection, counting multiple occurrences.
- Example

| Word        | Collection Frequency      | Document Frequency      |
| ----------- | ------------------------- | ----------------------- |
| *insurance* | 10,440                    | 3,997                   |
| *try*       | 10,422                    | 8,760                   |
|             | How many times in general | In how many documents ? |

- Which word is a better search term (and should get a higher weight)?
  - insurance --> because in less document and still have a same general frequency, occurs more in a given document

### tf-idf weighting

- The tf-idf weight of a term is the product of its tf weight and its idf weight.

$$
w_{t,d} = (1+\log_{10}tf_{t,d}) x \log_{10}(N/df_{t})
$$

- ==Best known weighting scheme in IR==
  - Note: the "-" in tf-idf is a hyphen, not a minus sign
  - Alternative names: tf.idf, tf x idf
- Increases with the **number of occurrences within a document**
- ==Increases with the rarity of term in the collection==

### Final ranking of documents for a query

$$ {\LARGE}
Score(q,d) = \sum_{t \in q \cap d} tf.idf_{t,d}
$$

- tf-idf score of a query-document pair is the sum of tf-idf scores of common words in query and document

## Binary --> Count --> Weight Matrix

![image-20201120125528191](../../../AppData/Roaming/Typora/typora-user-images/image-20201120125528191.png)

Each document is now represented by a real-valued
vector of tf-idf weights ∈ R^|V|^

### Documents as vectors

- So we have a |V|-dimensional vector space
- **Terms are axes of the space**
- Documents are points or vectors in this space
- ==Very high-dimensional: tens of millions of dimensions when you apply this to a web search engine==
- These are very sparse vectors - most entries are zero

### Queries as vectors

- **Key idea 1:** Do the same for queries: represent them as vectors in space
- **Key idea 2:** Rank documents according to their proximity to the query in this space
  - **Proximity = similarity of vectors**
  - ​                 **&asymp; inverse of distance**
- Recall: We do this because we want to get away from the Boolean model
- Instead: rank **more relevant documents higher** than less relevant documents

![image-20201120130444679](../../../AppData/Roaming/Typora/typora-user-images/image-20201120130444679.png)



### Formalizing vector space proximity

- First cut: distance between two points
  - Distance between the end points of the two vectors
- Euclidean distance?
- Euclidean distance is a bad idea...
  - because Euclidean distance is **large** for vectors of **different lengths**.

![image-20201120165709171](../../../AppData/Roaming/Typora/typora-user-images/image-20201120165709171.png)

### Use angle instead of distance

- ==Thought experiment:==
  - Take a document *d*
  - Append it to itself *d'* --> |*d'*| = 2.|*d*| 
- Semantically d and *d'* have the same content
- ==The Euclidean distance between the two documents can be quite large==
- The **angle between the two documents is 0**, corresponding to maximal similarity

- ==Key idea: Rank documents according to angle with query==

## From angles to cosines

- The following two notions are equivalent
  - Rank documents in <u>increasing</u> order of the angle between query and document
  - Rank documents in <u>decreasing</u> order of **cosine(query, document)**
- Cosine is a monotonically decreasing function for the interval [0&deg; , 90&deg;]

![image-20201120172519121](../../../AppData/Roaming/Typora/typora-user-images/image-20201120172519121.png)



### Length Normalization

- A vector can be (length-) normalized by dividing each of its components by its length - for this we use L~2~ norm:

$$
||\vec{x}||_{2} = \sqrt{\sum x_i^2}
$$

- 
- ==Diving a vector by its L~2~ norm makes it a unit(length) vector (on surface of unit hypersphere)==
- Think of a vector (3,4)
  - For each x axis
  - $\sqrt {x^2 +y^2}$ = 5
  - Then divide each axis value by the length
  - (3/5, 4/5)
  - For each axis of the document (each term)
  - Log freq weight normalization == $ logweight_t/ \sqrt{\sum_{t \in d} logweight_{i}^2 }$
- Effect on the two documents d and d' --> they have identical vectors after length-normalization.
  - ==Long and short documents now have comparable weights.==

## Cosine Similarity

### cosine(query, document)

$$
\begin{align}
\cos(\vec{q}, \vec{d}) &= \frac{\vec{q}\cdot \vec{d}}{\vec{q}.\vec{d}}\\
         			   &= \frac{{\vec{q}}}{|{\vec{q}|}}\cdot \frac{\vec{d}}{|{\vec{d}|}}\\
         			   &=  \frac{{\sum_{i=1}^vq_id_i}}{{\sqrt{\sum_{i=1}^vq_i^2}}.{\sqrt{\sum_{i=1}^vd_i^2}}}\\
\end{align}
$$

- q~i~ is the tf-idf weight of term *i* in **the query**
- *d~i~* is the tf-idf weight of term *i* in **the document**

- cos($$\vec{q}$$, $$\vec{d}$$) is **the cosine similarity of $$\vec{q}$$ and $$\vec{d}$$** ... or, equivalently, the cosine of the angle between $$\vec{q}$$ and $$\vec{d}$$.

### Cosine for length-normalized vectors

- For length-normalized vectors, cosine similarity is simply the dot product (or scalar product):

$$
\cos(\vec{q},\vec{d}) = \vec{q} \cdot \vec{d} = \sum q_id_i
$$

​														        		for q, d length-normalized

#### Cosine similarity amongst 3 documents

- How similar are the novels
  - SaS: Sense and Sensibility
  - PaP: Pride and Prejudice
  - WH: Wuthering Heights

##### Term Frequency(counts)

| term      | SaS  | PaP  | WH   |
| --------- | ---- | ---- | ---- |
| affection | 115  | 58   | 20   |
| jealous   | 10   | 7    | 11   |
| gossip    | 2    | 0    | 6    |
| wuthering | 0    | 0    | 38   |

Note: To simplify this example, we don't use do idf weighting



##### Log frequency Weighting

| term      | SaS  | PaP  | WH   |
| --------- | ---- | ---- | ---- |
| affection | 3.06 | 2.76 | 2.30 |
| jealous   | 2.00 | 1.85 | 2.04 |
| gossip    | 1.30 | 0    | 1.78 |
| wuthering | 0    | 0    | 2.58 |



##### After length normalization

| term      | SaS   | PaP   | WH    |
| --------- | ----- | ----- | ----- |
| affection | 0.789 | 0.832 | 0.524 |
| jealous   | 0.515 | 0.555 | 0.465 |
| gossip    | 0.335 | 0     | 0.405 |
| wuthering | 0     | 0     | 0.588 |

- cos(SaS,PaP) &asymp;  0.789 * 0.832 + 0.515 * 0.555 + 0.335 * 0 + 0 * 0 

​						      &asymp; 0.94

- cos(SaS, WH) &asymp; 0.79

- cos(PaP, WH) &asymp; 0.69

Question  

- How to create the vector of a query since it only contains few words?
  - Is the query vector angle 



