# Evaluation of IR System (Cont)



### ROC Curve Example

- More left to the curve, much better

- In this example --> 14 docs

  - 5 relevant
  - 9 non-relevant

- 1-specificity  --> False Positive Rate 

  - For all retrieved non-relevant, how it accumulates
  - FP/ 9 non-relevant

- sensitivity --> Recall --> True Positive Rate 

  - For all retrieved relevant, how it accumulates
  - TP / 5

  

| n    | Relevant? | FPR       | TPR  |
| ---- | --------- | --------- | ---- |
| 1    | X         | 0         | 0,2  |
| 2    | X         | 0         | 0,4  |
| 3    |           | 1/9 ~ 0,1 | 0,4  |
| 4    | X         | 0,1       | 0,6  |
| 5    |           | 0,2       | 0,6  |
| 6    | X         | 0,2       | 0,8  |
| 7    |           | 0,3       | 0,8  |
| 8    |           | 0,4       | 0,8  |
| 9    |           | 0,5       | 0,8  |
| 10   |           | 0,6       | 0,8  |
| 11   |           | 0,7       | 0,8  |
| 12   |           | 0,8       | 0,8  |
| 13   | X         |           | 1,0  |
| 14   |           | 1,0       | 1,0  |



### Typical 11 point precisions (Interpolated cont.)

- TREC example
- Compute interpolated precisions at recall levels 0.0, 0.1, 0.2
- Do this for each of the queries in the evaluation benchmark
- Average over queries
- This measure measures performance **at all recall levels**
- The curve is typical of performance levels at TREC 



## Mean Average Precision (MAP)

- Mean average precision (MAP)
  - **AP for a query**: Average of the precision values obtained for the top *k* documents, each time a relevant doc is retrieved
  - MAP for query collection is **arithmetic average of the AP** values for all the queries
  - Each query is weighted equally
  - Most commonly used measure in the recent years
  - 


| n    | Relevant? | Precision | Recall |
| ---- | --------- | --------- | ------ |
| 1    | X         | **1,00**  | 0,2    |
| 2    | X         | **1,00**  | 0,4    |
| 3    |           | 0,67      | 0,4    |
| 4    | X         | **0,75**  | 0,6    |
| 5    |           | 0,60      | 0,6    |
| 6    | x         | **0,67**  | 0,8    |
| 7    |           | ..        | 0,8    |
| 8    |           |           | 0,8    |
| 9    |           |           | 0,8    |
| 10   |           |           | 0,8    |
| 11   |           |           | 0,8    |
| 12   |           |           | 0,8    |
| 13   | X         | **0,38**  | 1,0    |
| 14   |           | 1,0       | 1,0    |

- AP for a single query when k = 14
  - ( 1 + 1 + 0,75 + 0,67 + 0,38) / 5 = 0,76
- MAP for N queries: sum the APs of the queries and take the mean (SUM of APs) / N



## R-Precision

- If have known (through perhaps incomplete) **set of relevant document of size *Rel*,** then calculate precision of top *Rel* docs returned
- Perfect system could score 1.0 
- For the table above,
  - 5 Relevant documents
  - Precision at the top 5
  - R-precision = 3 / 5 = 0.6
  - Recall = 3/5 = 0.6
  - Also called **break-even point**:
    - point in the precision recall curve, where precision == recall

## Beyond Binary Relevance: Discounted Cumulative Gain

- Popular measure for evaluating web search and related tasks
- Two assumptions:
  - **Highly relevant documents are more usefu**l than marginally relevant documents
  - The **lower the ranked** position of a **highly relevant** document, the **less useful** it is for the user, since it is less likely to be examined

- Uses *graded relevance* as a measure of usefulness, or *gain* from examining a document
- *Gain* is accumulated starting at the top of the ranking and may be reduced or *discounted*, at lower ranks
- **Typical discount is 1/log(rank)**
  - With base 2, the discount at rank 4 is 1/2, and at rank 8 it is 1/3

### Summarize a Ranking: DCG

- What if relevance judgments are in scale of [0, r] ? r > 1
- Cumulative Gain (CG) at rank n
  - Let the ratings of the *n* documents be r~1~, r~2~, r~3~, ... ,r~n~ (in ranked order)
  - CG = r~1~ + r~2~ + ... r~n~
- Discounted Cumulative Gain (DGC) at rank *n*
  - DCG = r~1~ + ( r~2~ / log~2~2 ) + ( r~3~ / log~2~3 ) + ....  + (r~n~ / log~2~n)
  - We may use any base for the logarithm

### Discounted Cumulative Gain

- DCG is the total gain accumulated at a particular rank *p*

$$
DCG_p = rel_1 + \sum^{p}_{i=2} \dfrac{rel_i}{\log_2i}
$$

- Alternative formulation:

$$
DCG_p = \sum_{i=1}^{p} \dfrac{2^{rel_i}-1}{\log(1+i)}
$$

- Compute for each query and take average over all queries.
- Often **Normalized** **DCG** so that perfect ranking obtains a score of 1

#### DCG Example

- 10 ranked documents judges on 0-3 relevance scale

  - 3, 2, 3, 0, 0, 1, 2, 2, 3, 0
  - Discounted gain: rel / log(rank)
    - 3 log~2~1, 2 / log~2~2 , 2/log~2~3 0,0, 1/2.59, 2.....
    - 3, 2, 1.89, 0, 0, 0.39, 0.71, 0.67, 0.95, 0
  - DCG:
    - 3, 5, 6.89, 6.89, 7.28, 7.99, 8.66, 9.61, 9.61

  

### Normalized DCG

- Normalized Discounted Cumulative Gain (NDCG) at rank *n*
  - Normalize DCG at rank *n* by 
    - the DCG value at rank *n* of the ideal ranking
  - The ideal ranking would first return the documents with the highest relevance level, then the next highest relevance level, etc.
- Normalization useful for contrasting queries with varying numbers of relevant results
- NDCG is now quite popular in evaluating Web search

#### Example

$$
DCG_{GT} = 2 + (\dfrac{2}{\log_22} + {\dfrac{1}{\log_23}} + {\dfrac{0}{\log_24}}) = 4,6309 
\\
DCG_{RF_1} =  2 + (\dfrac{2}{\log_22} + {\dfrac{1}{\log_23}} + {\dfrac{0}{\log_24}}) =  4,6309 
\\
DCG_{RF_2} =   2 + (\dfrac{1}{\log_22} + {\dfrac{2}{\log_23}} + {\dfrac{0}{\log_24}}) =  4,2619 
\\
MaxDCG = DCG_{GT} = 4,6309
$$



- NDCG of a ranking function is the DCG~function~Â / DCG~MaxDCG~

$$
NDCG_{RF_2} = \dfrac{4,2619}{4,6309} = 0.9203
$$

